"""
Basic usage example for KnowledgeCore Engine.

This example demonstrates the complete RAG pipeline:
1. Load and process documents
2. Build a knowledge base
3. Ask questions and get answers with citations
"""

import asyncio
import os
from pathlib import Path
from typing import List

from knowledge_core_engine.core.config import RAGConfig
from knowledge_core_engine.pipelines.ingestion import IngestionPipeline
from knowledge_core_engine.pipelines.retrieval import RetrievalPipeline
from knowledge_core_engine.pipelines.generation import GenerationPipeline


async def load_documents(file_path: Path, config: RAGConfig):
    """Load and process a document into the knowledge base."""
    print(f"\nğŸ“„ Loading document: {file_path}")
    
    # Initialize ingestion pipeline
    ingestion = IngestionPipeline(config)
    await ingestion.initialize()
    
    # Process the document
    try:
        result = await ingestion.process_document(file_path)
        print(f"âœ… Successfully processed: {result['chunks_created']} chunks created")
        print(f"   Document ID: {result['document_id']}")
        return result
    except Exception as e:
        print(f"âŒ Error processing document: {e}")
        raise


async def query_knowledge_base(query: str, config: RAGConfig):
    """Query the knowledge base and get an answer."""
    print(f"\nğŸ” Query: {query}")
    
    # Initialize retrieval pipeline
    retrieval = RetrievalPipeline(config)
    await retrieval.initialize()
    
    # Retrieve relevant contexts
    contexts = await retrieval.retrieve(query, top_k=5)
    print(f"ğŸ“š Found {len(contexts)} relevant contexts")
    
    # Initialize generation pipeline
    generation = GenerationPipeline(config)
    await generation.initialize()
    
    # Generate answer
    result = await generation.generate(query, contexts)
    
    print(f"\nğŸ’¡ Answer:\n{result.answer}")
    
    if result.citations:
        print(f"\nğŸ“– Citations:")
        for citation in result.citations:
            print(f"   [{citation.index}] {citation.document_title} (p.{citation.page})")
    
    print(f"\nğŸ“Š Tokens used: {result.usage.get('total_tokens', 'N/A')}")
    
    return result


async def main():
    """Main example demonstrating complete RAG workflow."""
    print("=== KnowledgeCore Engine Usage Example ===")
    
    # Configuration
    config = RAGConfig(
        # LLM Configuration
        llm_provider="deepseek",  # or "qwen", "openai"
        llm_model="deepseek-chat",
        llm_api_key=os.getenv("DEEPSEEK_API_KEY"),  # Set your API key
        
        # Embedding Configuration
        embedding_provider="dashscope",  # Using Qwen embeddings
        embedding_model="text-embedding-v3",
        embedding_api_key=os.getenv("DASHSCOPE_API_KEY"),  # Set your API key
        
        # Vector Store Configuration
        vector_store_provider="chromadb",
        vector_store_path="./data/chroma_db",
        
        # Generation Settings
        temperature=0.1,  # Low temperature for factual answers
        max_tokens=2048,
        include_citations=True,
        
        # Extra Parameters
        extra_params={
            "language": "zh",  # Chinese language
            "chunk_size": 512,
            "chunk_overlap": 50,
            "enable_metadata_enhancement": True,
            "citation_style": "inline"
        }
    )
    
    # Example 1: Load a document
    print("\nğŸ“š Example 1: Loading Documents")
    
    # Create sample document if it doesn't exist
    sample_doc = Path("./data/sample_rag_intro.md")
    if not sample_doc.exists():
        sample_doc.parent.mkdir(parents=True, exist_ok=True)
        sample_doc.write_text("""
# RAGæŠ€æœ¯è¯¦è§£

## ä»€ä¹ˆæ˜¯RAGï¼Ÿ

RAGï¼ˆRetrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§ç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„AIæŠ€æœ¯ã€‚å®ƒé€šè¿‡åœ¨ç”Ÿæˆç­”æ¡ˆä¹‹å‰å…ˆä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œä»è€Œå¤§å¤§æé«˜äº†è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

## RAGçš„æ ¸å¿ƒä¼˜åŠ¿

### 1. å‡å°‘å¹»è§‰
ä¼ ç»Ÿçš„å¤§è¯­è¨€æ¨¡å‹å®¹æ˜“äº§ç”Ÿ"å¹»è§‰"ï¼Œå³ç”Ÿæˆçœ‹ä¼¼åˆç†ä½†å®é™…é”™è¯¯çš„ä¿¡æ¯ã€‚RAGé€šè¿‡åŸºäºçœŸå®æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆï¼Œæ˜¾è‘—é™ä½äº†è¿™ç§é£é™©ã€‚

### 2. çŸ¥è¯†å¯æ›´æ–°
ä¸éœ€è¦é‡æ–°è®­ç»ƒçš„ä¼ ç»Ÿæ¨¡å‹ä¸åŒï¼ŒRAGç³»ç»Ÿçš„çŸ¥è¯†åº“å¯ä»¥éšæ—¶æ›´æ–°ï¼Œæ— éœ€ä¿®æ”¹åº•å±‚æ¨¡å‹ã€‚

### 3. å¯è§£é‡Šæ€§å¼º
RAGç”Ÿæˆçš„æ¯ä¸ªç­”æ¡ˆéƒ½å¯ä»¥è¿½æº¯åˆ°å…·ä½“çš„æºæ–‡æ¡£ï¼Œæä¾›äº†æ¸…æ™°çš„å¼•ç”¨é“¾ï¼Œå¢å¼ºäº†ç­”æ¡ˆçš„å¯ä¿¡åº¦ã€‚

### 4. æˆæœ¬æ•ˆç›Šé«˜
ç›¸æ¯”äºè®­ç»ƒä¸“é—¨çš„é¢†åŸŸæ¨¡å‹ï¼ŒRAGä½¿ç”¨ç°æœ‰çš„é€šç”¨æ¨¡å‹é…åˆé¢†åŸŸçŸ¥è¯†åº“ï¼Œå¤§å¤§é™ä½äº†éƒ¨ç½²æˆæœ¬ã€‚

## RAGçš„å·¥ä½œæµç¨‹

1. **æ–‡æ¡£å¤„ç†**ï¼šå°†åŸå§‹æ–‡æ¡£è§£æã€åˆ†å—ï¼Œå¹¶è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
2. **æ£€ç´¢é˜¶æ®µ**ï¼šæ ¹æ®ç”¨æˆ·æŸ¥è¯¢ï¼Œä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£å—
3. **ç”Ÿæˆé˜¶æ®µ**ï¼šå°†æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸æŸ¥è¯¢ä¸€èµ·è¾“å…¥LLMï¼Œç”Ÿæˆå‡†ç¡®çš„ç­”æ¡ˆ
4. **å¼•ç”¨æ ‡æ³¨**ï¼šåœ¨ç­”æ¡ˆä¸­æ ‡æ³¨ä¿¡æ¯æ¥æºï¼Œæä¾›å¯è¿½æº¯æ€§

## ä¼ä¸šåº”ç”¨åœºæ™¯

- **çŸ¥è¯†ç®¡ç†ç³»ç»Ÿ**ï¼šæ„å»ºä¼ä¸šå†…éƒ¨çŸ¥è¯†é—®ç­”ç³»ç»Ÿ
- **å®¢æˆ·æœåŠ¡**ï¼šåŸºäºäº§å“æ–‡æ¡£çš„æ™ºèƒ½å®¢æœ
- **æ³•å¾‹å’¨è¯¢**ï¼šåŸºäºæ³•è§„æ–‡æ¡£çš„åˆè§„æ€§æŸ¥è¯¢
- **åŒ»ç–—è¾…åŠ©**ï¼šåŸºäºåŒ»å­¦æ–‡çŒ®çš„è¯Šç–—å»ºè®®

## å®æ–½RAGçš„å…³é”®è€ƒè™‘

å®æ–½RAGç³»ç»Ÿéœ€è¦è€ƒè™‘å¤šä¸ªæŠ€æœ¯è¦ç´ ï¼š
- æ–‡æ¡£è§£æçš„å‡†ç¡®æ€§
- åˆ†å—ç­–ç•¥çš„ä¼˜åŒ–
- å‘é‡åŒ–æ¨¡å‹çš„é€‰æ‹©
- æ£€ç´¢ç®—æ³•çš„è°ƒä¼˜
- ç”Ÿæˆæ¨¡å‹çš„é…ç½®
- æ•´ä½“ç³»ç»Ÿçš„æ€§èƒ½ä¼˜åŒ–

## æ€»ç»“

RAGæŠ€æœ¯ä»£è¡¨äº†AIåº”ç”¨çš„ä¸€ä¸ªé‡è¦æ–¹å‘ï¼Œå®ƒé€šè¿‡ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„ä¼˜åŠ¿ï¼Œä¸ºä¼ä¸šæä¾›äº†ä¸€ç§å®ç”¨ã€å¯é ã€å¯è§£é‡Šçš„AIè§£å†³æ–¹æ¡ˆã€‚éšç€æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒRAGå°†åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚
""", encoding="utf-8")
    
    # Load the document
    await load_documents(sample_doc, config)
    
    # Example 2: Ask questions
    print("\nğŸ’¬ Example 2: Asking Questions")
    
    questions = [
        "ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿå®ƒçš„å…¨ç§°æ˜¯ä»€ä¹ˆï¼Ÿ",
        "RAGç›¸æ¯”ä¼ ç»Ÿå¤§è¯­è¨€æ¨¡å‹æœ‰å“ªäº›ä¼˜åŠ¿ï¼Ÿ",
        "ä¼ä¸šå¯ä»¥åœ¨å“ªäº›åœºæ™¯ä½¿ç”¨RAGæŠ€æœ¯ï¼Ÿ",
        "å®æ–½RAGç³»ç»Ÿéœ€è¦è€ƒè™‘å“ªäº›æŠ€æœ¯è¦ç´ ï¼Ÿ"
    ]
    
    for question in questions:
        await query_knowledge_base(question, config)
        print("\n" + "="*50)
    
    # Example 3: Streaming generation (if needed)
    print("\nğŸŒŠ Example 3: Streaming Answer Generation")
    
    query = "è¯¦ç»†è¯´æ˜RAGçš„å·¥ä½œæµç¨‹"
    retrieval = RetrievalPipeline(config)
    await retrieval.initialize()
    contexts = await retrieval.retrieve(query, top_k=3)
    
    generation = GenerationPipeline(config)
    await generation.initialize()
    
    print(f"ğŸ” Query: {query}")
    print("ğŸ’¡ Answer (streaming):")
    
    accumulated_answer = ""
    async for chunk in generation.stream_generate(query, contexts):
        if chunk.content:
            print(chunk.content, end="", flush=True)
            accumulated_answer += chunk.content
        
        if chunk.is_final and chunk.citations:
            print(f"\n\nğŸ“– Citations:")
            for citation in chunk.citations:
                print(f"   [{citation.index}] {citation.document_title}")
    
    print("\n\nâœ¨ Example completed!")


if __name__ == "__main__":
    # Run the example
    asyncio.run(main())