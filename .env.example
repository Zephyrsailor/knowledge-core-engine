# KnowledgeCore Engine Environment Variables
# 复制此文件为 .env 并填入您的配置

# ========== LLM 配置 ==========
# 选择一个 LLM 提供商并配置相应的 API Key

# DeepSeek (推荐，成本低效果好)
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# 通义千问 (阿里云 DashScope)
DASHSCOPE_API_KEY=your_dashscope_api_key_here

# OpenAI (可选)
OPENAI_API_KEY=your_openai_api_key_here

# 默认 LLM 配置
LLM_PROVIDER=deepseek  # 可选: deepseek, qwen, openai
LLM_MODEL=deepseek-chat  # 或 qwen2.5-72b-instruct, gpt-4-turbo-preview

# ========== 嵌入模型配置 ==========
# 默认使用通义千问的嵌入模型
EMBEDDING_PROVIDER=dashscope  # 可选: dashscope, openai
EMBEDDING_MODEL=text-embedding-v3

# ========== 文档解析配置 ==========
# LlamaParse API Key (可选，提供1000次/天免费额度)
LLAMA_CLOUD_API_KEY=your_llama_parse_key_here

# ========== 向量数据库配置 ==========
# ChromaDB (默认，无需额外配置)
VECTOR_STORE_PROVIDER=chromadb
VECTOR_STORE_PATH=./data/chroma_db

# Pinecone (可选)
# PINECONE_API_KEY=your_pinecone_api_key_here
# PINECONE_ENVIRONMENT=us-east-1-aws
# PINECONE_INDEX_NAME=knowledge-core

# Weaviate (可选)
# WEAVIATE_URL=http://localhost:8080
# WEAVIATE_API_KEY=your_weaviate_api_key_here

# ========== 重排序模型配置 ==========
# 使用 HuggingFace 模型进行重排序
RERANKER_MODEL=BAAI/bge-reranker-v2-m3

# ========== API 服务器配置 ==========
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=1

# ========== 日志配置 ==========
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR
LOG_FILE=./logs/k-engine.log

# ========== 性能配置 ==========
MAX_CONCURRENT_REQUESTS=10
REQUEST_TIMEOUT=60  # 秒
BATCH_SIZE=32

# ========== 缓存配置 ==========
ENABLE_CACHE=true
CACHE_TTL=3600  # 秒
CACHE_DIR=./data/cache

# ========== 其他配置 ==========
# 语言设置
DEFAULT_LANGUAGE=zh  # zh 或 en

# 开发模式
DEBUG=false
RELOAD=false